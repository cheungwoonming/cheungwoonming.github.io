---
bg: "2022/2022-12-bg.jpg"
layout: post
title:  "模型层面知识点"
crawlertitle: "模型层面知识点"
summary: "模型层面知识点总结"
date: 2022-12-08
categories: posts
tags: ['面试知识点']
author: Accelzhang
---

## 生成模型和判别模型：
生成模型（ Generative Modeling）有：朴素贝叶斯模型，隐马尔科夫模型，生成对抗网络等，对于不可见的X，需要计算它和不同分类的数据之间的联合概率分布，最大者获胜。
判别模型（ Discriminative Modeling）有：线性回归，支持向量机SVM，决策树等，通过输入属性X直接得到y，一个概率或者得分，然后用一个边界（比如0.5）区划分类别。

**GM特点**：生成方法学习联合分布P(X,Y)，所以就可以从统计的角度表示数据的分布情况，能够反映同类数据本身的相似度。但它不关心到底划分各类的那个分类边界在哪。生成方法可以原出联合概率分布分布P(X,Y)，而判别方法不能。生成方法的学习收敛速度更快，即当样本容量增加的时候，学到的模型可以更快的收敛于真实模型，当存在隐变量时，仍可以用生成方法学习，但判别方法就不能用。

**DM特点**：判别方法直接学习的是决策函数Y=f(X)或者条件概率分布P(Y|X)，不能反映训练数据本身的特性。但它寻找不同类别之间的最优分类面，反映的是异类数据之间的差异。直接面对预测，往往学习的准确率更高。由于直接学习P(Y|X)或f(X)，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题
## SVM
**SVM的核函数**：对于线性不可分的数据，通过核函数将数据映射到一个更高的维度，使数据在这个高位空间中是线性可分的。 https://blog.csdn.net/vincent2610/article/details/52035370

**SVM推导过程**：给定样本集，在样本空间中找到一个超平面将不同的类别划分开来，先假设有这么个超平面（w,b）wT*b=0，将样本的x带入到（w,b）中，通过判断结果的正负号来判断分类，假设分类是正负1，那么可以将预测结果和正式结果相乘，由相乘结果的正负来判断这两类是否分类正确，将这个相乘的表达式定义为函数间隔，为了避免这个间隔随（w,b）的变化而变化，添加一个w的二阶范数（类似模），也使得svm自带正则化功能，就变成了几何间隔，这个几何间隔要么大于等于1，要么小于等于1，几何间隔正好为正负1的样本点就是支持向量，正负样本的支持向量到这个超平面的几何间隔之和越大，分类的置信度就越高，问题就转化为在满足分类正确的情况下最大化这个几何间隔。再通过引入拉格朗日乘子转化为对偶变量的优化问题，满足KKT条件的情况下，对偶问题的解就是原问题的解，这里也可以引入核函数进而推广到非线性分类问题。https://blog.csdn.net/v_july_v/article/details/7624837

**LR和SVM的相同点**：

1. 如果不考虑核函数，两者都是线性分类函数，分类决策都是找到一个超平面来划分样本
2. 都是有监督的分类算法，都属于判别模型

**不同点**：
1. LR是基于概率理论，用sigmoid函数划分，而SVM是间隔最大化原理，认为存在一个最大几何分类面
2. SVM只考虑局部边界线附近的点（即支持向量），而LR受所有数据点影响
3. SVM可以通过核函数解决非线性问题，而LR只能通过特征构造引入非线性
4. SVM是用了结构风险最小化算法，自动正则项，而LR需要在损失函数上添加正则项

## 决策树
为了找到当前结点的最优划分特征，可以基于信息熵，信息增益，信息增益率，基尼指数等选取：

熵代表了变量的确定性，混乱程度，对于预测目标y，在得知某个特征x之后，y的条件熵可能会下降，而选取的时候就是选取能够使y的熵下降最多的那个特征，这个熵值差就是这个特征x的信息增益。信息增益率就是会再除以y原本的熵

**回归树和分类树的区别**：分类树在选取特征做划分节点时，是基于特征的信息增益等选取的，并且通过节点内所有样本投票决定这个节点的类别；而回归树是根据节点内所有样本的均值作为这个节点的预测值，用划分后节点的预测值和真实值之前的均方根误差来选择特征。

**随机森林和GBDT的区别**：
1. 随机森林是bagging思想，GBDT是boosting思想，bagging是有放回的均匀随机采样，而boosting是根据上一轮分类器错误率去修改样本的权重，从而改变了错误率高的在下一轮被采样的概率更大，bagging是可并行的而boosting是必须串行的。
2. 随机森林采用多数投票等方法，而GBDT是所有结果累加的，或者加权累加的。
3. 随机森林对异常值不敏感，GBDT对异常值非常敏感。
[参考文档][随机森林和GBDT的区别]

**XGBoost和LightGBM的区别**：
1. lgb是基于Histogram的决策树算法（所以消耗内存更少）
2. 带深度限制的Leaf-wise的叶子生长策略，只分裂降低loss最多的叶子节点。（对比xgb的Lever-wise分裂策略，每一层都会分裂，最后再剪枝）
3. 直方图做差加速直接（加速运行）
4. 支持类别特征(Categorical Feature)
5. Cache命中率优化
6. 基于直方图的稀疏特征优化多线程优化。

## 神经网络
**Batch normalization**：
神经网络中的各层输入会随着前面几层的变化而变化，包括数据的分布，从而导致模型再训练中还得去适应不同的分布，模型训练效率降低，收敛的更慢，而BN就是在每个网络的输入层之前，做一个归一化处理，保证每层网络的输入分布式固定的，让模型的学习更加专注。

**BP算法中的链式法则**：
就是求复合函数的偏导数，对于多层（深度）神经网络而言，它的数学本质，就等同于一个多层的复合函数：
[![image]({{site.images}}/2022/2022-12-08-1.png)]({{site.images}}/2022/2022-12-08-1.png)
根据链式法则：
[![image]({{site.images}}/2022/2022-12-08-2.png)]({{site.images}}/2022/2022-12-08-2.png)
[参考文档][BP算法中的链式法则]

**LSTM原理详解**：
两种状态：长期状态和短期状态
三个开关：长期状态是否流入到下一时刻，当前短期状态是否输入到长期状态中，长期状态是否输入当前当前状态的计算中。
而GRU同样是两个状态，但只有两个开关（更新门和重置门），，模型比lstm简单，参数量减少了1/3，不容易过拟合
[参考文档][LSTM原理详解]

**deepFM原理**：
其实是Wide&Deep和FM的结合，与Wide&Deep不同，deepFM两部分共享稀疏输入和稠密输入，同时在wide部分改造成FM，FM中分为两部分：线性部分和交叉部分，线性部分即特征直接乘以权重矩阵进行输出，交叉部分是对特征进行两两交叉再乘以权重矩阵进行输出，两部分进行累加合成一个输出；Deep部分就是对稀疏输入和稠密输入进行多层线性和非线性的转化，最后累加合成一维输出，和Wide进行累加再sigmoid。

**注意力机制**：
从seq2seq中来，已经输入序列的Embedding矩阵K=K1, K2,…,Kn和输出序列V=V1, V2,…,Vm，K通过编码器之后输出一个隐向量Q，需要用Q通过解码器来重构输出序列，在注意力机制之前，seq2seq是只用Q向量直接重构得到输出序列。注意力机制就是用K矩阵和Q向量进行计算（各种计算方式），得到Q向量在K矩阵每个item的注意力权重w，通过这个权重与V向量对应相乘就得到最后输出序列，就是在计算每一个输出item时，同时考虑Q向量和K矩阵每个item向量对当前输入的影响权重。
自注意力机制：就是K矩阵和V矩阵是相同输入构造得到，相当于关注自己与自己每一个位置的影响权重。
多头注意力机制：相当于卷积神经网络的多个卷积核，通过生成多个Q、K、V向量，计算多个注意力权重矩阵w，实现多种角度的特征提取，最后合并多个结果。
Layer Normalization：对单个样本的某一层输出进行标准化，BatchNorm是对一个batch的样本中对同一通道（同一特征维度）上的输出进行标准化。LayerNorm适用于NLP领域，因为NLP任务通常不考虑不同样本之间的大小关系，只考虑同一样本内不同特征之间的大小关系；BatchNorm适用于CV领域，需要保留不同样本之间的大小关系。
shape:[N, C, H, W]，N 代表batch长度，C代表通道数，H代表每层的隐藏单元数，W代表每层的权重；
BatchNorm是在batch上，对NHW做归一化；
LayerNorm在channel方向上，对CHW归一化；
InstanceNorm在单个通道像素上，对HW做归一化；
GroupNorm，有点类似LayerNorm,将channel分组，然后再做归一化
[![image]({{site.images}}/2022/2022-12-08-3.png)]({{site.images}}/2022/2022-12-08-3.png)


## Transformer 和 BERT ：
[参考文档][Transformer 和 BERT]
**transformer**：
由编码器（encoder）和解码器（decoder）组成，编码器的子结构是相同的，就是一个叠加的子结构，每个子结构包括两层：自注意力层和前馈神经网络。
[![image]({{site.images}}/2022/2022-12-08-4.png)]({{site.images}}/2022/2022-12-08-4.png)
首先输入的单词id序列，通过embedding转换为词向量，然后进入每一个子结构，即自注意力层和前馈神经网络层
自注意力机制：序列中不同位置的id对某一个位置的id的预测权重是不一样的，具体做法就是，将词向量与三个权重矩阵相乘得到三个向量：查询向量Q，键向量K，值向量V，对于一个位置上的id来说，用它的Q向量，乘以所有id的K向量，再经过softmax得到所有id对当前id的权重，权重再乘以当前id的V向量得到当前id的自注意力层输出；
而多头自注意力机制，就是训练多组不同的Q、K、V向量，把所有输出凭借在一起，再乘以另一个训练矩阵，压缩到一个小的维度。
还可以添加一个可训练的位置编码，它与词向量维度相同，直接跟词向量求和，再输入到编码器中。
还有一个可训练的语句编码，用来表示输入的前后两句话，也是直接跟词向量求和。
编码器的最后输出是K向量和V向量，将输送到解码器的每一个子结构，而上一时刻的解码器输出又会输入到下一时刻的解码器最底层的那个子结构，意味着解码器的每一个子结构（除了最底层的那个在第一个time step）都会有两个输入：上一个子结构的输出和编码器的输出。

**BERT模型**：
就是基于transformer的Encoder层，并对encoder进行了堆叠。
预训练任务有两个：
（1）masked language model：一句话中随机mask掉几个词，mask的操作是80%的概率替换为[mask]标识，10%的概率不变，10%的概率随机替换；词的定义有两种，一种是完整的单词，另一种是把一个完整单词划分为多个word piece，每个词块看做一个整体。用的是多分类交叉熵损失函数。
（2）next sentence prediction：判断两句话是否是前后连续的关系，用二分类交叉熵损失函数。
[更多详细关于BERT的介绍]

**BERT和ELMo的区别**：
ELMo有两个问题：
（1）虽然有正向和反向两个LSTM，但是是分开训练的，并非完全意义的双向LSTM；
（2）预测的id是已经存在的，很可能在模型中出现泄漏，自己预测自己。
BERT用的是transformer的encoder做特征提取，层数更高，并行性更好，真正意义上的双向学习，并且加入了self-attention，embedding学习效果更好，通过mask任务也没有泄漏的问题，并且充分学习了字符，词级，句子级和句间关系特征。

## 传统Item2User推荐系统框架
以电商场景为例
1. 推荐池，收集近30成交出现过的商品等；
2. 召回，基于一定的规则，针对user快速从推荐池中召回一定量的商品；通常为多路召回，包含个性化召回（user标签、userCF、itemCF、embedding召回、GraphSage）和非个性化召回（热门、高分、好评）
3. 粗排，从召回结果中筛选千级别item给精排模块，介于召回和精排之间，通常模型比较简单，整体速度比精排快，准确率比召回高；
4. 精排，对粗排结果重新打分排序，选取TopN进行推荐。精排重点在于负样本的选取，正样本很容易得到，但是负样本却很难选择。
直接使用曝光未点击样本，是明显不合适的，因为能曝光的item本身肯定与user有一定的相关性，相比全体推荐池里的item。而用随机负样本有一点的学习作用，但是也有问题，随机选择的样本很容易与正样本区分，需要难区分的负样本（hard negative sample）来提升模型效果。其中一种获取难区分负样本的方法，（Facebook EBR）就是用上一个召回模型打分处于中间位置的item，采样作为这一轮模型训练的负样本，同时混合一定比例的随机负样本（拟合实际分布），去训练模型。
[推荐算法架构]
精排模型：阿里的ESMM多任务学习模型，由于CVR本身和CTR没有任何关系（未点击的人未必不会转化），所以构建了一个CTCVR，即点击之后再转化的任务CTCVR=CTR*CVR，以CTR和CTCVR为有监督训练模型，间接训练了CVR任务。
[阿里云精排模型介绍]
[![image]({{site.images}}/2022/2022-12-08-5.png)]({{site.images}}/2022/2022-12-08-5.png)

**腾讯的RALM模型**：
同样是将item推荐给user，改为使用item曾经交互过的user序列，来生成描绘item的Embedding，然后再基于Embedding相似进行扩散推荐给距离最近的其他user。
具体实现：
1. 使用双塔结构的模型item&user，训练用户曝光点击数据，取最后一层网络输出作为user Embedding
2. 同样使用双塔结构的模型userlist&user，左边是item交互过的userlist，使用第一步训练得到的user Embedding特征进行聚合，右边是目标user 的user Embedding，经过FC网络之后再计算相似度，训练这个item与user的相关性。
3. 上述模型全部离线训练，取出user Embedding在线保存到内存，进行kv查取，然后第二步的模型也预先加载到内存，最后可实现在线快速推理。

[更多精排模型的演变过程]
[![image]({{site.images}}/2022/2022-12-08-6.png)]({{site.images}}/2022/2022-12-08-6.png)
[![image]({{site.images}}/2022/2022-12-08-7.png)]({{site.images}}/2022/2022-12-08-7.png)
[![image]({{site.images}}/2022/2022-12-08-8.png)]({{site.images}}/2022/2022-12-08-8.png)


[随机森林和GBDT的区别]: https://blog.csdn.net/qq_14997473/article/details/88877198
[BP算法中的链式法则]: https://zhuanlan.zhihu.com/p/44138371?utm_source=wechat_session
[LSTM原理详解]: https://blog.csdn.net/qq_31278903/article/details/88690959
[Transformer 和 BERT]: https://blog.csdn.net/weixin_42137700/article/details/107977119
[更多详细关于BERT的介绍]: https://github.com/NLP-LOVE/ML-NLP/tree/master/NLP/16.8%20BERT
[推荐算法架构]: https://zhuanlan.zhihu.com/p/497850386
[阿里云精排模型介绍]: https://developer.aliyun.com/article/847428?utm_content=g_1000316270
[更多精排模型的演变过程]: https://zhuanlan.zhihu.com/p/433135805
