---
bg_url: "https://cdn.jsdelivr.net/gh/cheungwoonming/picx_img@main/AI_img2/image-021.jpg"
layout: post
title:  "大模型知识汇总-截止到2025年8月"
crawlertitle: "大模型知识汇总-截止到2025年8月"
summary: "与大模型相关的基础知识、LLM训练相关知识、LLM4Rec应用及其他应用知识..."
date: 2025-08-24
categories: posts
tags: ['一千零一页']
author: Accelzhang
---


# （1）基础
## transformer
- **输入**： 接受input id序列，转变为token的语义embedding序列，每个token再加上位置embedding的到最终的embedding序列。
- **encoders**：多个相同结构的encoder堆叠组成，每个encoder的数据流程均为：self-attention --> add&norm(残差连接) --> Feed Forward Network --> add&norm(残差连接)。
最终转变为另一种含义的embedding序列（同输入维度）。抽出最后一层的k和v向量，输送decoder。
- **decoders**：
  - 多个相同结构的decoder堆叠组长，每个decoder的数据流程均为：Mask self-attention --> encoder-decoder-attention --> add&norm(残差连接) --> Feed Forward Network --> add&norm(残差连接).
  - 生成过程：首先基于一个固定起始special-token，通过decoders转变为embedding，取最后一个token对应的embedding通过liner层转变为词表维度的一维向量，经过softmax作为概率，取概率最大的index对应的词进行输出。再将这个词加到刚开始的special-token后面，进行循环生成。
  - Mask self-attention（与encoder区别之一）是为了防止未来位置的token信息影响到当前位置token的生成，具体实现就是通过一个下三角矩阵作为掩码矩阵，其他位置为-∞，使得最终结果softmax为0。
  - encoder-decoder-attention（与encoder区别之一）的k和v来自encoders最后一层

## self-attention
- **QKV向量**：是由输入token的embedding向量经过三个QKV矩阵，得到的低纬度向量，QKV矩阵通常都为N*N的维度，N为输入序列token长度，因此序列越长QKV矩阵越大。
- 除以根号d的意义是，使QK之后的分布方差跟d无关，softmax不会呈现一种随d增加而突出陡峭的状态，训练更加稳定
  - [![image]({{site.images}}/2025/2025-08-24-1.png)]({{site.images}}/2025/2025-08-24-1.png)
- flash-attention：将QKV分成多个小块，每次只计算一小块的attention输出，再进行累加得到完整的attention结果，本质上是时间换空间，增加计算量，但能大幅降低显存，因为不需要保存一份完整的N*N矩阵。


## Norm
Norm的使用场景是：训练数据分布差异较大，模型在训练过程中不仅要根据训练目标学习，还要学习训练样本的差异，导致模型训练不稳定，难以收敛。
而通过Norm就能让数据分布变得简单，模型能够更加专注学习训练目标。而不同的Norm方式就是就是应对数据在不同维度上的分布差异
- layer-norm：对一个样本的不同维度特征进行归一化，通常这里的不同维度是具有重叠相似性，比如序列，才有归一化的意义，因此常用语NLP领域。transformer中使用的是layerNorm，但实现来说是对每个token的embedding进行归一化，因此其实是instance norm
- batch-norm：对batch内样本的同一组特征进行归一化，通常用于CV领域

更多详细可以看这个：https://zhuanlan.zhihu.com/p/12113221623

## 模型应用
- encoder模型：通常用于推荐领域
- decoder模型：通常用于问答生成领域，此时decoder需要删除encoder-decoder-attention层
- encoder-decoder模型：通常用于翻译领域

# （2）LLM

## SFT
- 微调方式：
  - **全参数微调**：全部模型参数都参与训练，显存需要加载全部参数，资源消耗最大，理论上性能最高，但可能导致灾难性遗忘，需要大量的训练数据。
  - **Lora**：在原来模型的FFN层旁边增加一个旁路，通过两个可训练小矩阵进行低秩分解（先降维后升维），将结果接到FFN层的输出中，来模拟参数更新。训练参数非常少（通常不到原模型1%），训练速度快，稳定性高。
    - [![image]({{site.images}}/2025/2025-08-24-2.png)]({{site.images}}/2025/2025-08-24-2.png)
  - **QLora**：在Lora的基础上，将原模型以4bit低精度方式储存，进一步降低显存消耗，但推理时需要反量化回16bit，因此训练时间会比Lora长一些。

- 训练加速
  - **deepspeed**：通过将优化器的状态、梯度、甚至权重参数在分布式环境中进行分割，降低显存消耗/提升显存利用率，用通信开销时间换取显存降低，适用于训练长序列/超大模型。
  - **megatron**：通过数据并行（dp）、模型并行（pp）和张量并行（tp）和硬件加速，能够极致提升计算效率，解决单GPU显存无法放入一个完整模型的问题。

- 推理加速
  - **vLLM**：通过pageAttention和内存共享优化，优化KV cache缓存来提升推理速度。详细：https://zhuanlan.zhihu.com/p/691038809
  - **降低推理精度**：Fp16，还可以进一步使用量化int8压缩，依赖英伟达显卡

- LLM生成随机性控制
  - **temperature**：影响softmax函数，softmax函数中的指数会除以这个t，t越大，则softmax之后的结果更加均衡，大小差异更小，softmax之后小值的概率变大了，从而随机性更高了。
    - 引入t前后的softmax函数：[![image]({{site.images}}/2025/2025-08-24-4.png)]({{site.images}}/2025/2025-08-24-4.png) vs [![image]({{site.images}}/2025/2025-08-24-5.png)]({{site.images}}/2025/2025-08-24-5.png)
    - [![image]({{site.images}}/2025/2025-08-24-3.png)]({{site.images}}/2025/2025-08-24-3.png)
  - **top-k**：每一步生成只考虑概率最高的k个token，再按照其概率进行采样，k越大随机性越高。
  - **top-p**：每一步生成将token按照概率从大到小排序，选择概率累计超过p的token，再按照其概率进行采样，同样p越大随机性越高
更多详细可以看这个：https://blog.csdn.net/2301_77727994/article/details/145903035

## RLHF（PPO）
- Actor模型：正在训练的LLM，基于prompt生成优化答案
- Reference模型：pretrain+sft之后的模型，同样基于prompt生成一个参考答案，用于约束Actor模型的生成答案分布不能偏离太多（KL散度）。
- Critic模型：对Actor模型生成的优化答案给出预估得分（predict label）
- Reward Model：对Actor模型生成的优化答案给出真实得分（true label）
- PPO训练过程：先利用偏好数据训练一个reward model，才能开始LLM训练；这是一种强化学习方式，采样一批prompt，生成答案，计算预估得分和真实得分，进而得到优势（梯度），叠加Reference模型的KL散度约束，对actor模型参数进行更新。
- 缺点：多次交互训练，较为复杂

## DPO
- 训练过程：不属于强化学习，对比PPO，砍掉了复杂的critic模型和reward model，将偏好数据直接应用到模型训练目标中，利用BT模型的思想，最大化chosen样本生成概率减去rejected样本生成概率，使模型尽可能生成chosen样本的偏好。也会有KL散度约束。
- 问题：实际训练过程中，训练目标只是扩大chosen样本和rejected样本的差距，但并不保证rejected样本损失的概率能转移到chosen样本的生成概率，甚至造成输出结果完全错误。因此通常还会增加一个对chosen样本的sft-loss

## GRPO
- 训练过程：属于强化学习，对比PPO，砍掉了critic模型，通过对一个prompt的多次采样答案得到多个reward，并取平均得到平均reward，用答案reward减去平均reward来代表每个答案的优势，同时将KL散度直接加到损失函数中进行优化。
- 优点：训练更加稳定，显存和计算资源都有所下降

## MOE模型
- 本质：代替attention之后FFN层的一种新网络结构，该结构能够只激活部分参数，就能计算得到下一层embedding，在不降低参数量的情况，能大幅降低计算量，从而提升训练和推理速度。
- 网络结构：
  - 专家网络（Expert Network）：多个FFN层，代表多个专家，所有专家都接收相同的输入，产生不同的输出。
  - 门控网络（Gating NetWork）：接收与专家相同的输入，产出不同专家的重要程度权重
  - 选择器（Selector）：根据专家权重选择专家的策略，可以选择topN个专家
- 核心需解决问题：
  - 负载均衡：由于选择器是根据专家重要程度进行选择的，可能存在大部分网络都由头部几个专家网络激活，导致少数专家通信繁忙并拥堵，而其他专家则训练不充分。
- deepseek解决方案：
  - 划分更细粒度的专家，并隔离出共享专家
  - 专家级的负载loss和设备级的负载loss（V1和V2版本）：针对loss进行修改，使模型输出如果覆盖更多专家或更多设备时，loss是更低的
  - 无辅助损失的负载均衡（V3版本）：不再用loss进行负载均衡（会增加与优化目标无关的计算量），而是通过bias来调节不同专家的激活概率

# （3）LLM4Rec

## 字节的HLLM
- 先训练一个基于item的LLM，获取item embedding。
- 再用item embedding去训练基于user的LLM，预测下一个感兴趣的item embedding，通过生成式推荐loss和判别式推荐loss共同训练。

## 小红书的noteLLM
- 用prompt的方式，组装一个item笔记信息，然后输出一个代表压缩信息的special token。
- 训练目标有三个，一个是协同共现item的相似性loss，一个标签的sft loss，一个是类别的sft loss，共通去训练训练生成的token embedding，进行i2i召回。

## 快手的oneRec
- 第一步，通过一个多模态表征模型将视频映射为很多组向量，再通过Qformer将多组向量压缩为4组向量，再用RQ-kmeans将向量映射为语义ID。
- 第二步，将用户信息和各种行为序列输入到LLM模型，进行生成式推荐任务的训练。
- 第三步，用生成结果上线曝光，拿到点击数据进行一个类似精排模型的训练，作为一个reward model，再结合其他格式reward等模型，对LLM进行grpo的RL训练。
  - 优化后的GRPO：有个提前裁剪操作防止梯度爆炸，使训练更加稳定，同时砍掉了KL散度的loss，让模型一直日更。

## 快手oneSug
- 第一步，将前缀和query进行embedding化，然后用前缀相关的query对前缀的embedding进行表征增强，感觉类似一种修正，再通过RQ-vae生成语义id，最后就基于语义id进行检索召回topk。
- 第二步，将用户信息和前缀输入LLM，进行query的推荐召回，sft任务
- 第三步，DPO训练进行用户偏好对齐，dpo做了一定的优化：
  - 将pair-wise改为list-wise，充分利用偏好序列数据。
  - 引入了一个行为权重，对不同的行为序列有不用权重reward，让dpo不仅能够学习预估值的排序，还能拟合预估值本身。



# （4）其他LLM应用

## RAG
- **检索阶段**：对用输入进行关键词提取、实体识别等，然后基于比如向量检索的方式，从预处理好的文档库中检索出相关的材料。
- **增强阶段**：对检索出来的材料进行去重、截断、排序等预处理。
- **生成阶段**：将处理好的检索信息，跟原问题一起输入到大模型中进行答案生成。

## 多模态LLM
- 模型结构：
  - 模态编码器：将多模态数据编码成向量空间特征，常用CNN的resNET、Transformer结构的ViT等；
  - 输入投影层：将上面得到的向量特征映射到LLM输入特征空间的适配层，通常较为简单，如MLP、Cross-Attention；
  - LLM主干网络：经过预训练的LLM网络，串联多个模块一起SFT，使LLM能够识别多模态输入的token或向量；
  - 输出投影层：将LLM生成数据映射到其他模态的特征，同样为简单的MLP或Tiny Transformer；
  - 模态生成器：基于上面特征生成其他模态的输出。如图像的Stable Diffusion，视频的Zeroscope，音频的AudioLDM；
- AE、VAE、VQ-VAE、RQ-Kmeans

[![image](https://cdn.jsdelivr.net/gh/cheungwoonming/picx_img@main/AI_img2/image-021.jpg)](https://cdn.jsdelivr.net/gh/cheungwoonming/picx_img@main/AI_img2/image-021.jpg)
