---
bg_url: "https://cdn.jsdelivr.net/gh/cheungwoonming/picx_img@main/AI_img2/image-021.jpg"
layout: post
title:  "大模型知识汇总-截止到2025年8月"
crawlertitle: "大模型知识汇总-截止到2025年8月"
summary: "最近刷"
date: 2025-08-24
categories: posts
tags: ['一千零一页']
author: Accelzhang
---


# （1）基础
## transformer
- **输入**： 接受input id序列，转变为token的语义embedding序列，每个token再加上位置embedding的到最终的embedding序列。
- **encoders**：多个相同结构的encoder堆叠组成，每个encoder的数据流程均为：self-attention --> add&norm(残差连接) --> Feed Forward Network --> add&norm(残差连接)。
最终转变为另一种含义的embedding序列（同输入维度）。抽出最后一层的k和v向量，输送decoder。
- **decoders**：
  - 多个相同结构的decoder堆叠组长，每个decoder的数据流程均为：Mask self-attention --> encoder-decoder-attention --> add&norm(残差连接) --> Feed Forward Network --> add&norm(残差连接).
  - 生成过程：首先基于一个固定起始special-token，通过decoders转变为embedding，取最后一个token对应的embedding通过liner层转变为词表维度的一维向量，经过softmax作为概率，取概率最大的index对应的词进行输出。再将这个词加到刚开始的special-token后面，进行循环生成。
  - Mask self-attention（与encoder区别之一）是为了防止未来位置的token信息影响到当前位置token的生成，具体实现就是通过一个下三角矩阵作为掩码矩阵，其他位置为-∞，使得最终结果softmax为0。
  - encoder-decoder-attention（与encoder区别之一）的k和v来自encoders最后一层

## self-attention：
- **qkv**：是由输入token的embedding向量经过三个qkv矩阵，得到的低纬度向量。
- 除以根号d的意义是，使QK之后的分布方差跟d无关，softmax不会呈现一种随d增加而突出陡峭的状态，训练更加稳定
[![image]({{site.images}}/2025/2025-08-24-1.png)]({{site.images}}/2025/2025-08-24-1.png)

## 模型应用
- encoder模型：通常用于推荐领域
- decoder模型：通常用于问答生成领域，此时decoder需要删除encoder-decoder-attention层
- encoder-decoder模型：通常用于翻译领域

# LLM

## LLM生成随机性控制
- **temperature**：影响softmax函数，soft-max函数中的指数会除以这个t，将原本的输入结果变得更加均衡，大小差异更小，softmax之后小值的概率变大了，从而随机性更高了。
- **top-k**：
- **top-p**：

# LLM4Rec


Norm层
https://zhuanlan.zhihu.com/p/12113221623


[![image](https://cdn.jsdelivr.net/gh/cheungwoonming/picx_img@main/AI_img2/image-021.jpg)](https://cdn.jsdelivr.net/gh/cheungwoonming/picx_img@main/AI_img2/image-021.jpg)
